# -*- coding: utf-8 -*-
"""Đồ án lập trình phân tích dữ liệu nhóm 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oA-AGyY_jtJsaxFW8EJiNmimQ03a6dXC
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

# %cd /content/drive/My Drive/Đồ án lập trình phân tích DL
!ls

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import datetime as dt

from datetime import datetime, date

data = pd.read_excel('Raw_data.xlsx')

"""#Xử lý dữ liệu trong phân khúc đối tượng khách hàng (Customer Demographic)"""

# Tải dữ liệu đối tượng khách hàng (Customer Demographic)
cust_demo = pd.read_excel('Raw_data.xlsx' , sheet_name='CustomerDemographic')
cust_demo.head(5)

cust_demo.shape

# Thông tin về các cột và kiểu dữ liệu của Dữ liệu phân khúc Đối tượng Khách hàng
cust_demo.info()

cust_demo.describe()

"""Kiểu dữ liệu của các cột không có nhiều vấn đề. Tuy nhiên, ở đây cột 'default' là một cột không liên quan và không cần thiết, nên tiến hành loại bỏ/ xóa nó khỏi tập dữ liệu.

##Tổng số các bản ghi (records)
"""

print("Tổng số các bản ghi (các hàng) trong tập dữ liệu : {}".format(cust_demo.shape[0]))
print("Tổng số các cột (features) trong tập dữ liệu : {}".format(cust_demo.shape[1]))

"""## Các cột Numeric và các cột Non-Numeric"""

# Chọn các cột số (numeric)
df_numeric = cust_demo.select_dtypes(include=[np.number])
numeric_cols = df_numeric.columns.values
print("Các cột có dữ liệu là số : {}".format(numeric_cols))


# Chọn các cột không phải là số (non - numeric)
df_non_numeric = cust_demo.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values
print("Các cột có dữ liệu không phải là số : {}".format(non_numeric_cols))

"""## Loại bỏ các cột không liên quan

Cột 'default' là cột không liên quan. Vì vậy, nó nên được loại bỏ.
"""

# Loại bỏ cột default
cust_demo.drop(labels={'default'}, axis=1 , inplace=True)

# Mô tả dữ liệu
cust_demo[['past_3_years_bike_related_purchases', 'tenure']].describe()

"""##Kiểm tra Missing Values

Kiểm tra missing values trong tập dữ liệu. Nếu có giá trị missing cho một thuộc tính cụ thể, tùy thuộc vào tình hình, thuộc tính có thể bị loại bỏ (trong trường hợp mất mát một lượng lớn dữ liệu) hoặc một giá trị thích hợp sẽ được điền vào cột thuộc tính có missing value.
"""

# Tổng số giá trị còn thiếu
cust_demo.isnull().sum()

# % Missing Values
cust_demo.isnull().mean()*100

"""Ở đây, quan sát thấy rằng các cột như last_name, DOB, job_title, job_industry_category và tenure có missing values.

## Last Name
"""

# Kiểm tra sự có mặt của first name và customer ID trong các bản ghi
cust_demo[cust_demo['last_name'].isnull()][['first_name', 'customer_id']].isnull().sum()

"""Vì tất cả khách hàng đều có customer_id và First name, nên tất cả khách hàng đều có thể xác định. Do đó, không sao nếu không có last name. Điền giá trị null cho last name bằng "None"."""

# Truy xuất các bản ghi (last_name)
cust_demo[cust_demo['last_name'].isnull()]

cust_demo['last_name'].fillna('None',axis=0, inplace=True)

cust_demo['last_name'].isnull().sum()

"""Hiện tại không có missing value cho cột last name

## Date of Birth
"""

cust_demo[cust_demo['DOB'].isnull()]

round(cust_demo['DOB'].isnull().mean()*100)

"""Vì ít hơn 5% dữ liệu có ngày sinh là null, nên có thể loại bỏ các bản ghi có ngày sinh là null."""

dob_index_drop = cust_demo[cust_demo['DOB'].isnull()].index
dob_index_drop

cust_demo.drop(index=dob_index_drop, inplace=True, axis=0)

cust_demo['DOB'].isnull().sum()

"""Hiện tại không có giá trị thiếu cho cột Ngày sinh (DOB)

## Tạo cột Age kiểm tra sự không nhất quán trong dữ liệu
"""

import pandas as pd

# Convert the 'DOB' column to datetime.date objects
cust_demo['DOB'] = pd.to_datetime(cust_demo['DOB']).dt.date

# Define the age function
def age(born):
    today = date.today()
    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))

# Calculate the age of customers
cust_demo['Age'] = cust_demo['DOB'].apply(age)

plt.figure(figsize=(10,10))
sns.distplot(cust_demo['Age'], kde=False, bins=50)

"""###Thống kê của cột Age"""

cust_demo['Age'].describe()

"""Chỉ có 1 khách hàng có tuổi là 179. Rõ ràng đây là một giá trị ngoại lai vì phần trăm thứ 75 của Tuổi là 55."""

cust_demo[cust_demo['Age'] > 100]

"""
Ở đây, một khách hàng có tuổi là 179, đó là một giá trị ngoại lai. Do đó,  cần loại bỏ record này."""

age_index_drop = cust_demo[cust_demo['Age']>100].index

cust_demo.drop(index=age_index_drop, inplace=True , axis=0)

"""## Tenure

Khi Date of Birth là Null, Tenure cũng là Null. Do đó, sau khi loại bỏ Date of Birth là null khỏi bộ dữ liệu, tenure là null cũng đã bị loại bỏ.
"""

cust_demo['tenure'].isnull().sum()

plt.figure(figsize=(10,10))
sns.displot(cust_demo['tenure'], kde=False)

"""Không có missing value ở cột Tenure.

## Job Title
"""

# Truy xuất các records Job Title có missing values
cust_demo[cust_demo['job_title'].isnull()]

"""Vì tỷ lệ phần trăm của Job Title bị thiếu là 13%, chúng ta sẽ thay thế giá trị null bằng "Missing"
"""

cust_demo['job_title'].fillna('Missing', inplace=True, axis=0)

cust_demo['job_title'].isnull().sum()

"""Hiện tại không có missing value nào cho cột "job_title"

## Job Industry Category
"""

cust_demo[cust_demo['job_industry_category'].isnull()]

"""Vì tỷ lệ phần trăm của Job Industry Category là 16%, sẽ thay thế giá trị null bằng "Missing"
"""

cust_demo['job_industry_category'].fillna('Missing', inplace=True, axis=0)

cust_demo['job_industry_category'].isnull().sum()

"""Cuối cùng không có Missing values trong tập dữ liệu"""

cust_demo.isnull().sum()

print("Tổng số các bảng ghi sau khi loại bỏ missing values: {}".format(cust_demo.shape[0]))

"""## Kiểm tra sự không nhất quán trong dữ liệu

### Gender
"""

cust_demo['gender'].value_counts()

"""Ở đây có dữ liệu không nhất quán trong cột Gender. Có những lỗi chính tả và sai sót. Đối với giới tính có giá trị 'M', sẽ được thay thế bằng 'Male', 'F' và 'Femal' sẽ được thay thế bằng "Female'."""

def replace_gender_names(gender):

    if gender=='M':
        return 'Male'
    elif gender=='F':
        return 'Female'
    elif gender=='Femal':
        return 'Female'
    else :
        return gender

cust_demo['gender'] = cust_demo['gender'].apply(replace_gender_names)

cust_demo['gender'].value_counts()

"""Dữ liệu không nhất quán, lỗi chính tả và sai sót trong cột Gender đã được loại bỏ.

### Wealth Segment

Không có dữ liệu không nhất quán trong cột 'wealth_segment'
"""

cust_demo['wealth_segment'].value_counts()

"""### Deceased Indicator

Không có dữ liệu không nhất quán trong cột 'deceased_indicator'
"""

cust_demo['deceased_indicator'].value_counts()

"""### Owns a Car

Không có dữ liệu không nhất quán trong cột 'owns_car'
"""

cust_demo['owns_car'].value_counts()

"""## Kiểm tra Duplication

Chúng ta cần đảm bảo không có sự trùng lặp các records trong tập dữ liệu. Điều này có thể dẫn đến lỗi trong phân tích dữ liệu do chất lượng dữ liệu kém. Nếu có các hàng dữ liệu trùng lặp, chúng ra cần loại bỏ những records đó. Để kiểm tra các records trùng lặp, trước tiên chúng ta cần loại bỏ cột khóa chính của tập dữ liệu, sau đó áp dụng hàm 'drop_duplicates() được cung cấp bởi Python.
"""

cust_demo_dedupped = cust_demo.drop('customer_id', axis=1).drop_duplicates()

print("Lượng bản ghi sau khi xóa customer_id (pk), trùng lặp : {}".format(cust_demo_dedupped.shape[0]))
print("Lượng bản ghi trong tập dữ liệu gốc : {}".format(cust_demo.shape[0]))

"""Vì cả hai số đều giống nhau. Không có bản record trùng lặp trong dữ liệu.

##Xuất tập dữ liệu làm sạch Customer Demographic ra tệp CSV
"""

cust_demo.to_csv('CustomerDemographic_Cleaned.csv', index=False)

"""#Làm sạch dữ liệu Customer Address"""

cust_address = pd.read_excel('Raw_data.xlsx' , sheet_name='CustomerAddress')

cust_address.shape

cust_address.head(5)

cust_address.info()

"""##Tổng số các bản ghi (Records)"""

print("Tổng số các bản ghi (các hàng) trong tập dữ liệu : {}".format(cust_address.shape[0]))
print("Tổng số các cột(features) trong tập dữ liệu : {}".format(cust_address.shape[1]))

"""##Các cột số và các cột không phải là số"""

# Chọn các cột numeric
df_numeric = cust_address.select_dtypes(include=[np.number])
numeric_cols = df_numeric.columns.values
print("các cột có dữ liệu là số : {}".format(numeric_cols))


# Chọn các cột non-numeric
df_non_numeric = cust_address.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values
print("Các cột có dữ liệu không phải là số  : {}".format(non_numeric_cols))

"""## kiểm tra Missing Values


"""

# Tổng số các missing values
cust_address.isnull().sum()

"""Không có missing values trong cột dữ liệu.

## Kiểm tra sự không nhất quán trong dữ liệu

Chúng ta sẽ kiểm tra xem có dữ liệu không nhất quán/lỗi chính tả trong các cột phân loại hay không.
Các cột cần kiểm tra là 'address', 'postcode' ,'state', 'country'.

###State
"""

cust_address['state'].value_counts()

"""Ở đây có dữ liệu không nhất quán trong cột 'state'. Đối với New South Wales và Victoria, chúng ta có hai giá trị, một là tên đầy đủ và một là tên ngắn. Tên của các bang nên được chuẩn hóa và các cột có 'state' là New South Wales sẽ được thay thế bằng NSW và các cột với 'state' là Victoria sẽ được thay thế bằng VIC."""

# Hàm để thay thế tên đầy đủ của các bang bằng các biểu thức rút gọn của chúng
def replace_state_names(state_name):

    # Tạo biểu thức rút gọn của state_name theo tiêu chuẩn
    if state_name=='New South Wales':
        return 'NSW'
    elif state_name=='Victoria':
        return 'VIC'
    else :
        return state_name

# Áp dụng hàm trên vào cột 'state'
cust_address['state'] = cust_address['state'].apply(replace_state_names)

"""Sau khi áp dụng hàm trên, tên các bang đã được chuẩn hóa và không còn sự không nhất quán trong cột 'state'."""

cust_address['state'].value_counts()

"""### Country"""

cust_address['country'].value_counts()

"""Không có sự không nhất quán trong dữ liệu của cột 'country'

###Postcode

Cột 'postcode' trông hoàn hảo. Không có sự không nhất quán/lỗi chính tả trong dữ liệu.
"""

cust_address[['address','postcode', 'state' , 'country']].drop_duplicates()

"""###Kiểm tra sự trùng lặp"""

# Dropping the primary key column i.e customer_id and storing into a temporary dataframe.
cust_address_dedupped = cust_address.drop('customer_id', axis=1).drop_duplicates()

print("Tổng số các bản ghi sau khi loại bỏ customer_id (pk), duplicates : {}".format(cust_address_dedupped.shape[0]))
print("Tổng số các bản ghi trong dữ liệu gốc : {}".format(cust_address.shape[0]))

"""Không có record nào bị trùng lặp"""

cust_address.to_csv('CustomerAddress_Cleaned.csv', index=False)

"""###Kiểm tra số lượng records Master-Detail"""

cust_demo_detail = pd.read_csv('CustomerDemographic_Cleaned.csv')
cust_demo_detail.head()

print("Tổng số các bản ghi trong Customer_Demographic_Table : {}".format(cust_demo_detail.shape[0]))
print("Tỏng số các bản ghi trong Customer_Address_Table : {}".format(cust_address.shape[0]))
print('Trong bảng Demographic có {} bản ghi đang bị loại bỏ do quá trình làm sạch dữ liệu trong bảng Demographic'
      .format(cust_address.shape[0]-cust_demo_detail.shape[0]))

"""###Các Customer IDs trong bảng Address bị loại bỏ"""

cust_drop = cust_address.merge(cust_demo_detail , left_on = 'customer_id', right_on='customer_id'
                     , how='outer')
cust_drop.head()

"""# Làm sạch dữ liệu NewCustomerList"""

new_cust = pd.read_excel('Raw_data.xlsx' , sheet_name='NewCustomerList')
new_cust.head(5)

new_cust.info()

"""Kiểu dữ liệu của các cột đặc trưng là đúng. Tuy nhiên, các cột 'Unnamed: 16','Unnamed: 17','Unnamed: 18','Unnamed: 19','Unnamed: 20' không liên quan và nên được loại bỏ.

##Tổng số các bản ghi
"""

print("Tổng số các bản ghi (các hàng) trong tập dữ liệu : {}".format(new_cust.shape[0]))
print("Tổng số các cột (features) trong tập dữ liệu: {}".format(new_cust.shape[1]))

"""## Các cột số và các cột không phải số"""

# Chọn các cột numeric
df_numeric = new_cust.select_dtypes(include=[np.number])
numeric_cols = df_numeric.columns.values
print("Các cột có dữ liệu là số :")
print(numeric_cols)


# Chọn các cột non-numeric
df_non_numeric = new_cust.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values
print("Các cột có dữ liệu không phải là số :")
print(non_numeric_cols)

"""##Loại bỏ các cột không liên quan

'Unnamed: 16','Unnamed: 17','Unnamed: 18','Unnamed: 19','Unnamed: 20' là các cột không liên quan nên tiến hành loại bỏ chúng.
"""

new_cust.drop(labels=['Unnamed: 16','Unnamed: 17','Unnamed: 18','Unnamed: 19','Unnamed: 20'], axis=1 , inplace=True)

"""## kiểm tra Missing Values"""

# Tổng số Missing Values
new_cust.isnull().sum()

# % của Missing Values
new_cust.isnull().mean()*100

"""### Last Name

Vì tất cả khách hàng đều có first name, tất cả khách hàng có thể được xác định. Do đó, không có vấn đề nếu không có tên họ. Điền giá trị null của tên họ bằng 'None'.
"""

new_cust[new_cust['last_name'].isnull()][['first_name']].isnull().sum()

new_cust[new_cust['last_name'].isnull()]

new_cust['last_name'].fillna('None',axis=0, inplace=True)

new_cust['last_name'].isnull().sum()

"""Hiện tại, không có missing values nào cho cột "Last Name".

### Date of Birth
"""

new_cust[new_cust['DOB'].isnull()]

round(new_cust['DOB'].isnull().mean()*100)

"""Dưới 5% dữ liệu có ngày sinh là null. Chúng ta có thể loại bỏ các records mà ngày sinh là null"""

# FTruy xuất index của các records/cột nơi ngày sinh là null (DOB)

dob_index_drop = new_cust[new_cust['DOB'].isnull()].index
dob_index_drop

new_cust.drop(index=dob_index_drop, inplace=True, axis=0)

new_cust['DOB'].isnull().sum()

"""DOB không có missing values.

## Tạo cột Age để kiểm tra sự không nhất quán khác trong dữ liệu.
"""

# Hàm để tìm tuổi của khách hàng tính đến ngày hôm nay

def age(born):
    today = date.today()

    return today.year - born.year - ((today.month, today.day) < (born.month, born.day))

new_cust['Age'] = new_cust['DOB'].apply(age)

"""##Thống kê mô tả của cột Age"""

new_cust['Age'].describe()

# biểu đồ để xác định phân phối tuổi
plt.figure(figsize=(15,8))
sns.distplot(new_cust['Age'], kde=False, bins=50)

"""## Tạo cột nhóm Tuổi"""

import math

new_cust['Age Group'] = new_cust['Age'].apply(lambda x : (math.floor(x/10)+1)*10)

# Viz to find out the Age Group Distribution
plt.figure(figsize=(10,8))
sns.distplot(new_cust['Age Group'], kde=False, bins=50)

"""Số lượng lớn nhất khách hàng mới đến từ nhóm tuổi 50-59.

### Job Title
"""

new_cust[new_cust['job_title'].isnull()]

"""Vì tỷ lệ phần trăm của giá trị thiếu cho Job Title là 11%, chúng ta sẽ thay thế giá trị null bằng Missing"""

new_cust['job_title'].fillna('Missing', inplace=True, axis=0)

new_cust['job_title'].isnull().sum()

"""Hiện tại, không có missing values nào cho cột 'Job Title'.

### Job Industry Category
"""

new_cust[new_cust['job_industry_category'].isnull()]

"""Vì tỷ lệ phần trăm giá trị thiếu cho Job Industry Category là 16%, chúng ta sẽ thay thế giá trị nulll bằng "Missing"
"""

new_cust['job_industry_category'].fillna('Missing', inplace=True, axis=0)

new_cust['job_industry_category'].isnull().sum()

"""Hiện tại không có missing values cho cột Job Industry Category."""

new_cust.isnull().sum()

print("Tổng số các bản ghi sau khi loại bỏ missing values: {}".format(new_cust.shape[0]))

"""## Kiểm tra sự không nhất quán trong dữ liệu

Kiểm tra xem có dữ liệu không nhất quán/lỗi chính tả trong các cột phân loại không.
Các cột cần kiểm tra là 'gender', 'wealth_segment' ,'deceased_indicator', 'owns_car'

### Gender
"""

new_cust['gender'].value_counts()

"""### Wealth Segment"""

new_cust['wealth_segment'].value_counts()

"""### Deceased Indicator"""

new_cust['deceased_indicator'].value_counts()

"""### Owns a Car"""

new_cust['owns_car'].value_counts()

"""### State"""

new_cust['state'].value_counts()

"""### Country"""

new_cust['country'].value_counts()

"""### Postcode"""

new_cust[['postcode', 'state']].drop_duplicates().sort_values('state')

"""### Address"""

new_cust[['address', 'postcode','state','country']].sort_values('address')

"""### Tenure"""

new_cust['tenure'].describe()

# Distributon of tenure

plt.figure(figsize=(15,8))
sns.distplot(new_cust['tenure'])

"""Không có dữ liệu không nhất quán trong cột 'gender', 'wealth_segment', 'deceased_indicator', 'owns_car'.

## kiểm tra Duplication
"""

new_cust_dedupped = new_cust.drop_duplicates()

print("Tổng số các bản ghi sau khi loại bỏ customer_id (pk), duplicates : {}".format(new_cust_dedupped.shape[0]))
print("Tổng số các bản ghi trong dữ liệu gốc : {}".format(new_cust.shape[0]))

"""Vì cả hai số đều giống nhau. Không có bản records trùng lặp trong tập dữ liệu."""

new_cust.to_csv('NewCustomerList_Cleaned.csv', index=False)

"""# Làm sạch dữ liệu biến Transactions"""

plt.style.use('ggplot')

trans = pd.read_excel('Raw_data.xlsx' , sheet_name='Transactions')
trans.head(5)

# Quan sát các biến của bộ dữ liệu
trans.columns

# Thông tin về các cột và kiểu dữ liệu của biến Transactions
trans.info()

"""Kiểu dữ liệu của cột 'product_first_sold_date' không phải là định dạng datetime. Kiểu dữ liệu của cột này phải được thay đổi từ int64 sang định dạng datetime.

## Tổng số các bản ghi
"""

# Kích thước bộ dữ liệu
print("Tổng số các bản ghi (các hàng) trong tập dữ liệu : {}".format(trans.shape[0]))
print("Tổng số các cột (features) trong tập dữ liệu : {}".format(trans.shape[1]))

"""## Các cột Numeric và các cột Non-Numeric"""

# Chọn các cột numeric
df_numeric = trans.select_dtypes(include=[np.number])
numeric_cols = df_numeric.columns.values
print("Các cột có dữ liệu là số:")
print(numeric_cols)


# Chọn các cột non-numeric
df_non_numeric = trans.select_dtypes(exclude=[np.number])
non_numeric_cols = df_non_numeric.columns.values
print("Các cột có dữ liệu không phải là số :")
print(non_numeric_cols)

# Mô tả các biến số
trans[['list_price', 'standard_cost']].describe()

"""## Kiểm tra Missing Values"""

# Tổng số missing values
trans.isnull().sum()

# % của missing values
trans.isnull().mean()*100

"""Ở đây quan sát thấy được rằng các cột như online_order, brand, product_line, product_class, product_size, standard_cost, product_first_sold_date có missing values.

### Online Order

Vì 1.8% các records có dữ liệu online_order bị thiếu, chúng ta có thể thực hiện phương pháp điền giá trị mode cho cột phân loại này.
"""

trans[trans['online_order'].isnull()]

most_freq_online_mode = trans['online_order'].mode()
most_freq_online_mode

"""Vì 1 là giá trị phổ biến nhất của online_order. Chúng ta sẽ thực hiện phương pháp điền giá trị mode cho giá trị phân loại này."""

trans['online_order'].fillna(1, inplace=True)

trans['online_order'].isnull().sum()

"""Cột online_order không có missing values

### Brand, Product Line, Product Class, Product Size, Standard Cost, Product First Sold Date

Khi brand null thì tất cả các giá trị cột khác với missing values, như 'product_line', 'product_class', 'product_size', 'standard_cost', 'product_first_sold_date' cũng là null. Ngoài ra, giá trị null này chiếm 1% của tập dữ liệu. Do đó, chúng ta có thể loại bỏ những records này.
"""

trans[trans['brand'].isnull()][['brand', 'product_line', 'product_class', 'product_size',
                                'standard_cost', 'product_first_sold_date']].drop_duplicates()

trans[trans['brand'].isnull()][['brand', 'product_line', 'product_class', 'product_size',
                                'standard_cost', 'product_first_sold_date']].shape[0]

records_to_drop = trans[trans['brand'].isnull()][['brand', 'product_line', 'product_class', 'product_size',
                                'standard_cost', 'product_first_sold_date']].index
records_to_drop

trans.drop(index=records_to_drop, axis=0, inplace=True)

"""Cuối cùng, không có missin value trong tập dữ liệu transaction."""

trans.isnull().sum()

print("Tổng số các bản ghi sau khi loại bỏ Missing Values: {}".format(trans.shape[0]))

"""## Tạo một feature mới "Profit"

Cột 'Profit' sẽ là sự khác biệt giữa giá niêm yết và giá tiêu chuẩn của một sản phẩm
"""

trans['Profit'] = trans['list_price']-trans['standard_cost']

# Dystribution of the Profit Column
plt.figure(figsize=(20,8))
sns.distplot(trans['Profit'])

"""## Kiểm tra sự không nhất quán trong dữ liệu

### Online Order
"""

trans['online_order'].value_counts()

"""### Order Status"""

trans['order_status'].value_counts()

trans[['order_status', 'online_order']].drop_duplicates()

"""### Product Line"""

trans['product_line'].value_counts()

"""### Product Class"""

trans['product_class'].value_counts()

"""### Product Size"""

trans['product_size'].value_counts()

"""### Brand"""

trans['brand'].value_counts()

"""## Kiểm tra Duplication"""

trans_dedupped = trans.drop('transaction_id', axis=1).drop_duplicates()

print("Tổng số bản ghi sau khi loại bỏ customer_id (pk), duplicates : {}".format(trans_dedupped.shape[0]))
print("Tổng số bản ghi trong dữ liệu gốc : {}".format(trans.shape[0]))

trans.to_csv('Transactions_Cleaned.csv', index=False)

"""# Thống kê dữ liệu phân khúc khách hàng theo mô hình RFM"""

# Đọc dữ liệu file Transactions_Cleaned và CustomerDemographic_Cleaned
trans = pd.read_csv('/content/drive/MyDrive/Đồ án lập trình phân tích DL/Transactions_Cleaned.csv')
cust = pd.read_csv('/content/drive/MyDrive/Đồ án lập trình phân tích DL/CustomerDemographic_Cleaned.csv')

#trans = pd.read_csv('/content/Transactions_Cleaned.csv')
#cust = pd.read_csv('/content/CustomerDemographic_Cleaned.csv')

# 5 dòng đầu tiên của bảng Transactions_Cleaned
trans.head()

print(f"Tổng số quan sát (số dòng) trong bảng Transaction: {trans.shape[0]}")
print(f"Tổng số thuộc tính (số cột) trong bảng Transaction: {trans.shape[1]}")

# 5 dòng đầu tiên của bảng CustomerDemographic_Cleaned
cust.head()

print(f"Tổng số quan sát (số dòng) trong bảng Customer Demographic: {cust.shape[0]}")
print(f"Tổng số thuộc tính (số cột) trong bảng Customer Demographic: {cust.shape[1]}")

# Gộp 2 bảng Transaction và Customer Demographic bằng cột customer_id
merged_trans_cust = pd.merge(trans, cust, left_on='customer_id'
                                        , right_on='customer_id', how = 'inner')

# 5 dòng đầu tiên của bảng gộp merged_trans_cust
merged_trans_cust.head()

print(f"Tổng số quan sát (số dòng) trong bảng gộp merged_tran_cust: {merged_trans_cust.shape[0]}")
print(f"Tổng số thuộc tính (số cột) trong bảng gộp merged_tran_cust: {merged_trans_cust.shape[1]}")

# Thông tin của bảng gộp merged_trans_cust
merged_trans_cust.info()

"""Cột transaction_date đang ở kiểu dữ liệu object chứ khổng phải datetime"""

# Chuyển cột transaction_date về kiểu datetime
merged_trans_cust['transaction_date'] = pd.to_datetime(merged_trans_cust['transaction_date'])

"""# **2. Xuất file CSV**

# **1. Xử lí bảng Phân khúc khách hàng theo mô hình RFM**
Mô hình RFM Segmentation (Recency, Frequency, Monetary) dựa trên hành vi mua hàng của khách hàng để phân khúc khách hàng. RFM phân khúc khách hàng bằng các câu hỏi như là "Lần gần nhất họ mua là khi nào?", "Tần suất mua hàng của họ", "Số tiền họ đã bỏ ra là bao nhiêu?". Việc phân khúc như thế sẽ giúp cho doanh nghiệp nâng cao dịch vụ chăm sóc khách hàng của mình và thực hiện các chiến lược tiếp thị hiệu quả.

*   Recency (R): Khách hàng nào đã mua hàng gần đây? Số ngày kể từ lần mua gần nhất của họ?
*   Frenquency (F): Khách hàng nào mua hàng của công ty thường xuyên? => Tổng số lần mua hàng
*   Monetary (M): Khách hàng nào bỏ ra số tiền nhiều nhất để mua hàng? => Tổng số tiền mua hàng
"""

# Đơn hàng gần đây nhất được thực hiện vào ngày
max_trans_date = max(merged_trans_cust['transaction_date']).date()
max_trans_date = datetime.strptime(str(max_trans_date), "%Y-%m-%d")
print(max_trans_date)

# Tạo bảng rfm
# transaction_date là recency
# transaction_id là frequency
# profit là montery
rfm = merged_trans_cust.groupby(['customer_id']).agg({'transaction_date': lambda delta_date : (max_trans_date - delta_date.max()).days,
                                                      'transaction_id' : lambda transaction_id : transaction_id.count(),
                                                      'Profit' : lambda profit : sum(profit)})
rfm

# Đổi tên các cột của bảng rfm
rfm.rename(columns={'transaction_date' : 'recency',
                    'transaction_id' : 'frequency',
                    'Profit' : 'monetary'} , inplace=True)

rfm

# Chia các cột recency, frequency và monetary thành tứ phân vị (min, 25%, 50%, 75% và max)
# Phân loại khách hàng vào các phân vị đó
rfm['rank_r'] = pd.qcut(rfm['recency'], 4, ['4', '3', '2', '1'])
rfm['rank_f'] = pd.qcut(rfm['frequency'], 4, ['1','2','3','4'])
rfm['rank_m'] = pd.qcut(rfm['monetary'], 4, ['1','2','3','4'])

rfm

# Tính điểm RFM (rfm_score)
# Quy định trọng số của rencency là 100
#                       frenquency là 10
#                       monetary là 1
rfm['rfm_score'] = (100 * rfm['rank_r'].astype(int)
                    + 10 * rfm['rank_f'].astype(int)
                    + rfm['rank_m'].astype(int))

rfm

# Dựa vào điểm RFM từ thấp đến cao
# Xếp hạng khách hàng theo Bronze, Silver, Gold và Platinum
rfm['customer_rank'] = pd.qcut(rfm['rfm_score'], 4, ['Bronze','Silver','Gold','Platinum'])

rfm

"""### **Gộp bảng RFM với 2 bảng Transactions và Customer Demographic**

"""

# Gộp bảng rfm và bảng merged_trans_cust bằng cột customer_id
trans_cust_rfm = pd.merge(merged_trans_cust, rfm, left_on='customer_id', right_on='customer_id', how='inner')

# Thông tin của bảng gộp
trans_cust_rfm.info()

"""Tạo cột Age group từ cột Age"""

trans_cust_rfm['Age_Group'] = trans_cust_rfm['Age'].apply(lambda x : (math.floor(x / 10) + 1) * 10)

"""**Phân khúc khách hàng dựa trên điểm RFM**
Thực hiện phân khúc khách hàng thành 11 nhóm chi tiết hơn so với phân loại bên trên (4 nhóm). Các nhóm đó là: Platinum Customers, Very Loyal, Becoming Loyal, Recent Customers, Potential Customers, Late Bloomer, Loosing Customers, High Risk Customers, Almost Lost Customers, Evasive Customers and Lost Customers.
"""

# Hàm phân loại
def cust_segmentation(cols):

  rfm_score = cols[0]

  if rfm_score >= 444:
      return 'Platinum Customer'
  elif rfm_score >=433 and rfm_score < 444:
      return 'Very Loyal'
  elif rfm_score >=421 and rfm_score < 433:
      return 'Becoming Loyal'
  elif rfm_score >=344 and rfm_score < 421:
      return 'Recent Customer'
  elif rfm_score >=323 and rfm_score < 344:
      return 'Potential Customer'
  elif rfm_score >=311 and rfm_score < 323:
      return 'Late Bloomer'
  elif rfm_score >=224 and rfm_score < 311:
      return 'Loosing Customer'
  elif rfm_score >=212 and rfm_score < 224:
      return 'High Risk Customer'
  elif rfm_score >=124 and rfm_score < 212:
      return 'Almost Lost Customer'
  elif rfm_score >=112 and rfm_score < 124:
      return 'Evasive Customer'
  else :
      return 'Lost Customer'

# Sử dụng hàm để phân loại bảng trans_cust_rfm
trans_cust_rfm['customer_segmentation'] = trans_cust_rfm[['rfm_score']].apply(cust_segmentation, axis=1)

trans_cust_rfm

# Hàm xếp hạng khách hàng theo cột customer_segmentation
def get_rank(cols):

    segment = cols[0]

    if segment == 'Platinum Customer':
        return 1
    elif segment == 'Very Loyal':
        return 2
    elif segment == 'Becoming Loyal':
        return 3
    elif segment == 'Recent Customer':
        return 4
    elif segment =='Potential Customer':
        return 5
    elif segment == 'Late Bloomer':
        return 6
    elif segment == 'Loosing Customer':
        return 7
    elif segment =='High Risk Customer':
        return 8
    elif segment == 'Almost Lost Customer':
        return 9
    elif segment == 'Evasive Customer':
        return 10
    else :
        return 11

# Sử dụng hàm để xếp hạng phân loại khách hàng cho bảng trans_cust_rfm
trans_cust_rfm['customer_segmentation_ranking'] = trans_cust_rfm[['customer_segmentation']].apply(get_rank, axis=1)

trans_cust_rfm

"""# **2. Xuất file CSV**"""

trans_cust_rfm.to_csv('Customer_Trans_RFM_Analysis.csv', index = False)

print(f"Số dòng trong dataset Customer_Trans_RFM_Analysis : {trans_cust_rfm.shape[0]}")

"""# **Phân tích dữ liệu Phân khúc khách hàng từ file đã xử lí**

# **Phân tích dữ liệu đơn biến**
"""

df = pd.read_csv('/content/drive/MyDrive/Đồ án lập trình phân tích DL/Customer_Trans_RFM_Analysis.csv')
df.head()

"""```
#nominal: online_order, order_status, brand,
product_line, owns_car, deceased_indicator, wealth_segment,job_industry_category,job_title,gender, first_name, last_name, customer_rank
#ordinal: rank_r, rank_f, rank_m, customer_segmentation_ranking,product_class, product_size, Age_Group, rfm_score,
customer_segmentation
#continious: standard_cost, 'product_first_sold_date',  list_price, Profit, Age, monetary
#discreate: past_3_years_bike_related_purchases, tenure, recency, DOB, transaction_date,frequency
```

## qualitative

###*nominal*
"""

variable_nominal = df[['online_order', 'order_status', 'brand',
                          'product_line', 'owns_car', 'deceased_indicator', 'wealth_segment','job_industry_category','gender',
                          'customer_rank']]  # Tạo danh sách các thuộc tính cần phân tích thể loại nominal

plt.figure(figsize=(20,20)) # kích thước đồ thị
plotno = 1 # Biến đếm

for col in variable_nominal: # Lặp qua từng cột
    if plotno<=13:    # số lượng đồ thị
        plt.subplot(5,3,plotno) # Vẽ đồ thị (4 hàng, 2 cột), plotno là số thứ tự của đồ thị
        sns.countplot(x=variable_nominal[col]) # Vẽ đồ thị
        plt.xlabel(col,fontsize=20)  # Đặt tên cho trục x và cỡ chữ là 20
    plotno+=1 # Tăng biến đếm

# Hiển thị đồ thị
plt.tight_layout()
plt.show()

"""###*ordinal*"""

variable_ordinal = df[['rank_r', 'rank_f', 'rank_m', 'customer_segmentation_ranking','product_class', 'product_size', 'Age_Group',
                          'customer_segmentation']]  # Tạo danh sách các thuộc tính cần phân tích thể loại ordinal

plt.figure(figsize=(20,20)) # kích thước đồ thị
plotno = 1 # Biến đếm

for col in variable_ordinal: # Lặp qua từng cột
    if plotno<=9:    # số lượng đồ thị
        plt.subplot(3,3,plotno) # Vẽ đồ thị (4 hàng, 2 cột), plotno là số thứ tự của đồ thị
        sns.countplot(x=variable_ordinal[col]) # Vẽ đồ thị
        plt.xlabel(col,fontsize=20)  # Đặt tên cho trục x và cỡ chữ là 20
    plotno+=1 # Tăng biến đếm

# Hiển thị đồ thị
plt.tight_layout()
plt.show()

"""##quatitative

###discrete
"""

variable_discrete = df[['standard_cost',  'list_price', 'Profit']]  # Tạo danh sách các thuộc tính cần phân tích
plt.figure(figsize=(20,20)) # kích thước đồ thị
plotno = 1 # Biến đếm

for col in variable_discrete: # Lặp qua từng cột
    if plotno<=4:    # số lượng đồ thị
      a=df.groupby(col)
      b=a.size().reset_index(name='counts')
      plt.subplot(2,2,plotno)
      sns.lineplot(x=b[col],y=b.counts)
      plt.xlabel(col,fontsize=20)  # Đặt tên cho trục x và cỡ chữ là 20
    plotno+=1

# Hiển thị đồ thị
plt.tight_layout()
plt.show()

"""###continious"""

## past_3_years_bike_related_purchases
#nhóm lại thành 5 bins
group_smallerthan20  = df[(df['past_3_years_bike_related_purchases'] <= 20) ]
group_21_40 = df[(df['past_3_years_bike_related_purchases'].between(21,50, inclusive=True))]
group_41_60 = df[(df['past_3_years_bike_related_purchases'].between(40,60, inclusive=True))]
group_61_80 = df[(df['past_3_years_bike_related_purchases'].between(60,80, inclusive=True))]
group_morethan80  = df[(df['past_3_years_bike_related_purchases'] > 80) ]

dk=[len(group_smallerthan20), len(group_21_40),len(group_41_60),len(group_61_80), len(group_morethan80) ]
l=['group_smallerthan20','group_21_40','group_41_60','group_61_80','group_morethan80']

## vẽ biều đồ tròn
plt.figure(figsize=(10,10))
plt.pie(dk, labels = l, autopct = '%.f%%', shadow = True, counterclock = False)
plt.title('Biểu đồ biểu diễn past_3_years_bike_related_purchases')
plt.legend(title = 'Ghi chú:', loc = 1)

#tạo data bao gồm cột tenure và counts (đếm dố lượng mỗi biến của tenure)
a=df.groupby('tenure')
b=a.size().reset_index(name='counts')
sns.lineplot(x=b.tenure,y=b.counts)
plt.xlabel('tenure',fontsize=20)  # Đặt tên cho trục x và cỡ chữ là 20

# Hiển thị đồ thị
plt.tight_layout()
plt.show()

"""##Phân bổ ngành nghề của khách hàng mới và khách hàng cũ"""

new_cust = pd.read_csv('/content/drive/MyDrive/Đồ án lập trình phân tích DL/NewCustomerList_Cleaned.csv')

#khách hàng cũ
plt.figure(figsize=(20,10))
plt.subplot(1,2,1)
sns.countplot(x='job_industry_category',data=df[~(df['job_industry_category']=='Missing')]) ##không quan tâm đến giá trị bị missing
plt.xlabel('Job Industry')
plt.ylabel('Number of Customers')
plt.title('Old Customers - Job Industry Customer Distribution')

#khách hàng mới
plt.subplot(1,2,2)
sns.countplot(x='job_industry_category',data=new_cust[~(new_cust['job_industry_category']=='Missing')]) ##không quan tâm đến missing
plt.xlabel('Job Industry')
plt.ylabel('Number of Customers')
plt.title('New Customers - Job Industry Customer Distribution')

plt.tight_layout()
plt.show()

"""# **Phân tích dữ liệu đa biến**"""

# df = pd.read_csv("/content/Customer_Trans_RFM_Analysis.csv")
df = pd.read_csv("/content/drive/MyDrive/Đồ án lập trình phân tích DL/Customer_Trans_RFM_Analysis.csv")

df1 = pd.read_csv("/content/drive/MyDrive/Đồ án lập trình phân tích DL/NewCustomerList_Cleaned.csv")

df2 = pd.read_csv("/content/drive/MyDrive/Đồ án lập trình phân tích DL/CustomerAddress_Cleaned.csv")

"""Số lượng xe đạp bán ra trong vòng 3 năm gần đây theo giới tính"""

## Bảng "Số lượng xe đạp bán ra trong vòng 3 năm gần đây theo giới tính"
bikes_by_gender = df.groupby('gender').agg({'past_3_years_bike_related_purchases' : sum}).reset_index()

total_bikes = df['past_3_years_bike_related_purchases'].sum()

bikes_by_gender['% of total'] = (bikes_by_gender['past_3_years_bike_related_purchases']/total_bikes) * 100;

bikes_by_gender

## Biểu đồ "Số lượng xe đạp bán ra trong vòng 3 năm gần đây theo giới tính"
x_ = bikes_by_gender['gender']
y_ = bikes_by_gender['% of total']

plt.figure(figsize = (4, 4))
plt.title('% số lượng xe đạp bán ra theo giới tính')
sns.barplot(x = x_, y = y_)
plt.xlabel('Giới tính')
plt.ylabel('% số lượng xe đạp bán ra')

plt.show()

"""Số lượng xe đạp bán ra theo dòng sản phẩm và giới tính"""

## Bảng "Số lượng xe đạp bán ra theo dòng sản phẩm và giới tính"
pivot = df.pivot_table(values = 'product_id', index = ['product_line', 'product_class']
                       , columns = ['gender'], aggfunc = np.size)

pivot

##
plt.figure(figsize= (12, 8))

plt.title('Số lượng xe đạp bán ra theo dòng sản phẩm và giới tính')
sns.heatmap(pivot, annot=True, fmt='d', cmap='Blues', cbar=True)
plt.xlabel('Giới tính')
plt.ylabel('Dòng sản phẩm xe đạp')

plt.show()

"""Số lượng xe đạp bán ra theo kích thước và độ tuổi"""

## Số lượng xe đạp bán ra theo kích thước và độ tuổi
pivot1 = df.pivot_table(index = 'product_size', columns = 'gender',
                        values = 'product_id', aggfunc = np.size)
pivot1

## Số lượng xe đạp bán ra theo kích thước và độ tuổi
bikes_by_size_gender = df.groupby(by = ['gender', 'product_size'])['product_id'].count()
bikes_by_size_gender = bikes_by_size_gender.reset_index()

bikes_by_size_gender

## Biểu đồ Số lượng xe đạp bán ra theo kích thước và độ tuổi
x_ = bikes_by_size_gender['gender']
y_ = bikes_by_size_gender['product_id']
z_ = bikes_by_size_gender['product_size']

plt.figure(figsize = (10, 6))

plt.title('Số lượng xe đạp bán ra theo kích thước và độ tuổi')
sns.barplot(x = x_, y = y_, hue = z_)
plt.xlabel('Giới tính')
plt.ylabel('Số lượng xe đạp bán ra')

plt.show()

"""Số lượng đơn hàng theo phương thức đặt hàng và trạng thái đơn hàng"""

## Pivot "Số lượng đơn hàng theo phương thức đặt hàng và trạng thái đơn hàng"
pivot2 = df.pivot_table(index = 'online_order', columns = 'order_status',
                        values = 'transaction_id', aggfunc = np.size)
pivot2

## Bảng "Số lượng đơn hàng theo phương thức đặt hàng và trạng thái đơn hàng"
online_status_order = df.groupby(by = ['online_order', 'order_status'])['transaction_id'].count()
online_status_order = online_status_order.reset_index()

online_status_order

"""Số lượng khách hàng theo phân khúc độ giàu có và độ tuổi"""

## Bảng "Số lượng khách hàng cũ theo phân khúc độ giàu có và độ tuổi"
custs_by_wealth_age = df.groupby(by = ['wealth_segment', 'Age_Group']).size().reset_index(name = 'numbers_of_customers')

custs_by_wealth_age

## Biểu đồ "Số lượng khách hàng cũ theo phân khúc độ giàu có và độ tuổi"
x_ = custs_by_wealth_age['Age_Group']
y_ = custs_by_wealth_age['numbers_of_customers']
z_ = custs_by_wealth_age['wealth_segment']

plt.figure(figsize = (16, 8))
plt.title('Số lượng khách hàng cũ theo phân khúc độ giàu có và độ tuổi')
sns.barplot(x = x_, y = y_ , hue = z_)
plt.xlabel('Nhóm tuổi (Age Group)')
plt.ylabel('Số lượng khách hàng')

plt.show()

## Bảng "Số lượng khách hàng mới theo phân khúc độ giàu có và độ tuổi"
custs_by_wealth_age1 = df1.groupby(by = ['wealth_segment', 'Age Group']).size().reset_index(name = 'numbers_of_customers')

custs_by_wealth_age1

## Biểu đồ "Số lượng khách hàng mới theo phân khúc độ giàu có và độ tuổi"
x_ = custs_by_wealth_age1['Age Group']
y_ = custs_by_wealth_age1['numbers_of_customers']
z_ = custs_by_wealth_age1['wealth_segment']

plt.figure(figsize = (16, 8))
plt.title('Số lượng khách hàng mới theo phân khúc độ giàu có và độ tuổi')
sns.barplot(x = x_, y = y_ , hue = z_)
plt.xlabel('Nhóm tuổi (Age Group)')
plt.ylabel('Số lượng khách hàng')

plt.show()

"""Số lượng khách hàng theo từng thời kỳ"""

## Bảng "Số lượng khách hàng theo từng thời kỳ"
df['transaction_date'] = pd.to_datetime(df['transaction_date'])
df['transaction_month'] = df['transaction_date'].dt.month

month_cust = df.groupby('transaction_month').agg({'customer_id' : 'nunique'}).reset_index()
month_cust

## Biểu đồ "Số lượng khách hàng theo từng thời kỳ"
x_ = month_cust['transaction_month']
y_ = month_cust['customer_id']

plt.figure(figsize = (12, 6))
plt.title("Số lượng khách hàng theo từng thời kỳ")
sns.lineplot(x = x_, y = y_, marker = 'o')
plt.xlabel("Tháng, năm 2017")
plt.ylabel("Số lượng khách hàng")

plt.show()

"""Top 5 khách hàng mang ại lợi nhuận nhiều nhất cho công ty"""

## Top 5 khách hàng mang lại lợi nhuận nhiều nhất cho công ty
cust_profit = df.groupby('customer_id').agg({'Profit' : sum}).reset_index()

top5 = cust_profit.sort_values(by = 'Profit', ascending = False).head(5)
top5

"""Recency và Monetary"""

## Recency và Monetary
x_ = df['recency']
y_ = df['monetary']

plt.figure(figsize = (6, 6))
plt.title('Recency và Monetary')
plt.xlabel('Recency (ngày)')
plt.ylabel('Monetary ($)')
sns.scatterplot(x = x_, y = y_)

plt.show()

"""Frequency và Monetary"""

## Frequency và Monetary
x_ = df['frequency']
y_ = df['monetary']

plt.figure(figsize = (6, 6))
plt.title('Frequency và Monetary')
plt.xlabel('Frequency (lần)')
plt.ylabel('Monetary ($)')
sns.scatterplot(x = x_, y = y_)

plt.show()

"""Doanh thu, Chi phí và Lợi nhuận theo từng phân khúc khách hàng"""

## Doanh thu, Chi phí và Lợi nhuận theo từng phân khúc khách hàng
bus_cust = df.groupby(['customer_segmentation']).agg({'list_price': lambda total_sales : sum(total_sales),
                                                      'standard_cost' : lambda total_cost : sum(total_cost),
                                                      'Profit' : lambda profit : sum(profit)})

bus_cust.rename(columns={'list_price' : 'Total_Sales',
                         'standard_cost' : 'Total_Cost',
                         'Profit' : 'Total_Profit'} , inplace=True)

bus_cust = bus_cust.reset_index()
bus_cust

## Biểu đồ "Doanh thu, Chi phí và Lợi nhuận theo từng phân khúc khách hàng"
bus_cust1 = bus_cust.melt(id_vars = 'customer_segmentation', var_name = 'category', value_name = 'amount')

plt.figure(figsize=(20, 8))

# Plot the grouped bar chart
sns.barplot(x = 'customer_segmentation', y = 'amount', hue= 'category', data = bus_cust1)
plt.title('Doanh thu, Chi phí và Lợi nhuận theo từng phân khúc khách hàng')
plt.xlabel('Phân khúc khách hàng')
plt.ylabel('Doanh thu, Chi phí, Lợi nhuận ($)')
plt.legend(title='Category', loc='upper right')
plt.show()

"""Lợi nhuận mang lại cho công ty theo thương hiệu"""

## Lợi nhuận mang lại cho công ty theo thương hiệu
brand_profit = df.groupby('brand').agg({'Profit' : sum}).reset_index()

brand_profit

## Biểu đồ "Lợi nhuận mang lại cho công ty theo thương hiệu"
x_ = brand_profit['brand']
y_ = brand_profit['Profit']

plt.figure(figsize = (4, 4))
plt.title('Lợi nhuận mang lại cho công ty theo thương hiệu')
plt.pie(y_, labels = x_, autopct = '%.f%%')

plt.show()

"""# **Kiểm định tương quan**"""

data = pd.read_csv('/content/drive/MyDrive/Đồ án lập trình phân tích DL/Customer_Trans_RFM_Analysis.csv')

# Kiểm tra missing value
data.isnull().sum()

print(data.columns)

# Chọn các cột số (numeric)
data_numeric = data.select_dtypes(include=[np.number])
data_numeric_cols = data_numeric.columns.values
print("Các cột có dữ liệu là số : {}".format(data_numeric_cols))


# Chọn các cột không phải là số (non - numeric)
data_non_numeric = data.select_dtypes(exclude=[np.number])
data_non_numeric_cols = df_non_numeric.columns.values
print("Các cột có dữ liệu không phải là số : {}".format(data_non_numeric_cols))

import pandas as pd
from scipy.stats import chi2_contingency
non_numeric_cols = ['online_order', 'order_status', 'brand','product_line', 'owns_car', 'deceased_indicator',
                    'wealth_segment','job_industry_category','job_title','gender', 'first_name', 'last_name', 'customer_rank']
target_col = 'customer_segmentation'
high_corr_pairs = []
max_chi2 = 0

for col in non_numeric_cols:
    contingency_table = pd.crosstab(data[col], data[target_col])
    chi2, _, _, _ = chi2_contingency(contingency_table)
    # Điều kiện chọn cột có chi2 lớn hơn ngưỡng xác định
    if chi2 > 5.0:
        high_corr_pairs.append((col, chi2))
        if chi2 > max_chi2:
            max_chi2 = chi2

# In ra tất cả các cột có độ tương quan cao nhất với cột target
for col, chi2_val in high_corr_pairs:
    if chi2_val == max_chi2:
        print(f"Cột có độ tương quan cao nhất với target: {col} với giá trị chi-square là {chi2_val}")
    else:
        print(f"Cột: {col} có giá trị chi-square là {chi2_val}")

"""Chọn 10 biến phân loại có ảnh hưởng đến biến target là: brand, owns_car, deceased_indicator, wealth_segment, job_industry_category, job_title, gender, first_name, last_name, customer_rank.

Vì first_name và last_name là hai thuộc tính xác định khách hàng, nên có tương quan cao đối với phân khúc khách hàng nhưng không mang ý nghĩa trong quá trình xây dựng mô hình dư đoán phân khúc khách hàng nên ta có quyền loại bỏ chúng.
"""

del data ['first_name']
del data ['last_name']

ordinal_columns = ['rank_r', 'rank_f', 'rank_m', 'customer_segmentation_ranking', 'product_class',
                   'product_size', 'Age_Group', 'rfm_score', 'customer_segmentation']

# Tạo DataFrame từ các cột dữ liệu ordinal được chọn
ordinal_df = data[ordinal_columns]

# Hiển thị DataFrame chứa dữ liệu ordinal
print(ordinal_df)

# Kiểm định sự tương quan giữa các biến ordinal và biến target
from scipy.stats import spearmanr

data_subset = data[ordinal_columns]

# Thực hiện kiểm định Spearman's Rank-Order Correlation cho từng cặp biến
alpha = 0.05
for column in ordinal_columns[:-1]:
    corr, p_value = spearmanr(data_subset[column], data_subset['customer_segmentation'])

    print(f"Kiểm định tương quan Spearman's Rank-Order Correlation giữa {column} và customer_segmentation:")
    print(f"  - Spearman's correlation coefficient: {corr}")
    print(f"  - P-value: {p_value}")
    if p_value < alpha:
        print("  - Có tương quan đáng kể giữa hai biến.")
    else:
        print("  - Không có đủ bằng chứng để kết luận về tương quan giữa hai biến.")
    print("-------------------------------------------")

"""Các biến có tương quan với biến target là: rank_r, rank_m, rank_f, customer_segmentation_ranking, Age_Group, rfm_score."""

data_numeric = ['standard_cost', 'product_first_sold_date', 'list_price', 'Profit', 'Age', 'monetary',
                'past_3_years_bike_related_purchases', 'tenure', 'frequency']

plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['standard_cost'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['product_first_sold_date'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['list_price'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['Profit'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['Age'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['monetary'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['past_3_years_bike_related_purchases'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['tenure'], kde = True, hist = True)
plt.figure(figsize=(15, 15))
plt.subplot(3, 3, 1)
sns.distplot(data['frequency'], kde = True, hist = True)

# Vì bộ dữ liệu không tuân theo phân phối chuẩn nên ta chọn kiểm định Kruskal-Wallis Test
from scipy.stats import kruskal
data_numeric = ['standard_cost', 'product_first_sold_date', 'list_price', 'Profit', 'Age', 'monetary',
                'past_3_years_bike_related_purchases', 'tenure', 'frequency']
for column in data_numeric:
    groups = []
    for value in data['customer_segmentation'].unique():
        groups.append(data[data['customer_segmentation'] == value][column])

    # Thực hiện kiểm định Kruskal-Wallis
    stat, p_value = kruskal(*groups)

    # In kết quả
    print(f'Kruskal-Wallis Test cho biến {column} và customer_segmentation:')
    print(f'Chi-squared statistic: {stat}, p-value: {p_value}')
    if p_value < 0.05:  # Kiểm tra mức ý nghĩa thống kê (thường chọn alpha = 0.05)
        print("Có sự khác biệt ý nghĩa giữa các nhóm.")
    else:
        print("Không có sự khác biệt ý nghĩa giữa các nhóm.")
    print('-' * 50)

"""Các biến có ý nghĩa đối với biến target là: list_price, standard_cost, Profit, past_3_years_bike_related_purchases, tenure, Age, recency, frequency, monetary.

Vì customer_id là biến định danh dùng để phân biệt những khách hàng với nhau nên tương quan mạnh với biến customer_segmentation nhưng không có ý nghĩa trong quá trình dự đoán
=> Loại bỏ biến customer_id ra khỏi những biến cần phần tích tương quan với biến target
"""

del data ['customer_id']

"""# **Xây dựng mô hình dự đoán cho biến target 'customer_segmentation'**

# **Convert data**
"""

# Chuyển đổi order_status
data.order_status.value_counts()
data.order_status = data.order_status.map({'Approved':1,'Cancelled':0})
# kiểm tra chuyển đổi đã xảy ra hay chưa
data.order_status.value_counts()

data.product_line = data.product_line.map({'Standard':1,'Road':2,'Touring':3,'Mountain':4})
# kiểm tra chuyển đổi đã xảy ra hay chưa
data.product_line.value_counts()

data.owns_car = data.owns_car.map({'Yes':1,'No':0})
data.owns_car.value_counts()

data.deceased_indicator = data.deceased_indicator.map({'Y':1,'N':0})
data.deceased_indicator.value_counts()

data.wealth_segment = data.wealth_segment.map({'Mass Customer':1,'High Net Worth':2, 'Affluent Customer':3})
data.wealth_segment.value_counts()

data.job_industry_category = data.job_industry_category.map({'Manufacturing':1,'Financial Services':2,
                                                             'Missing':0,'Health':3,'Retail':4,'Property':5,'IT':6,
                                                            'Entertainment':7, 'Argiculture':8, 'Telecommunication':9})
data.job_industry_category.value_counts()

data.gender = data.gender.map({'Female':1,'Male':0})
data.gender.value_counts()

data.customer_rank = data.customer_rank.map({'Bronze':1,'Silver':2,
                                             'Gold':3,'Platinum':4})
data.customer_rank.value_counts()

data.product_class = data.product_class.map({'low':1,'medium':2,'high':3})
data.product_class.value_counts()

from sklearn.preprocessing import LabelEncoder

# Encode brand using label encoding
le = LabelEncoder()
data['brand'] = le.fit_transform(data['brand'])
data.brand.value_counts()
data['job_title'] = le.fit_transform(data['job_title'])
data.job_title.value_counts()

#chuyển male thành 1 và female thành 0
data.product_size = data.product_size.map({'small':1,'medium':2, 'large':3})
# kiểm tra chuyển đổi đã xảy ra hay chưa
data.product_size.value_counts()

"""# **Xử lí outliers**"""

# Kiểm tra outliers
out = data[['standard_cost', 'product_first_sold_date', 'list_price', 'Profit', 'Age', 'monetary',
                'past_3_years_bike_related_purchases', 'tenure', 'frequency']]
plotnumber = 1
plt.figure(figsize=(15, 10))
for column in out.columns:
    if plotnumber <= 7: # set the limit
          plt.subplot(3, 3, plotnumber)# # hình gồm 3 hàng và 3 cột, plotnumber là thứ tự mỗi hình nhỏ bên trong
          sns.boxplot(data=out, x=column)# Plotting box plots để phát hiện outlier
          plt.xlabel(column, fontsize=10)  # Thay đổi cỡ chữ tại đây
          plotnumber += 1
plt.tight_layout()
plt.show()

# Tính Q1 VÀ Q3 và iqr (tứ phân vị)
Q3, Q1 = np.percentile(data['standard_cost'], [75 ,25])
iqr = Q3-Q1
print(F'IQR = {iqr}\nQ1={Q1}\nQ3={Q3}')

# tính maximum và minimum limit
min_limit = Q1 - 1.5*iqr
print("Minimum limit:",min_limit)

max_limit = Q3 + 1.5*iqr
print("Maximum Limit:",max_limit)

##kiểm tra outliner
print(f"số outliers là: {data.loc[(data['standard_cost'] > max_limit) | (data['standard_cost'] < min_limit)].shape[0]}")

# Impute outlier (Cho outliner = median)
data.loc[data['standard_cost'] > max_limit,'standard_cost']=data['standard_cost'].median()
data.loc[data['standard_cost'] < min_limit,'standard_cost']=data['standard_cost'].median()

##kiểm tra outliner
print(f"số outliers là: {data.loc[(data['standard_cost'] > max_limit) | (data['standard_cost'] < min_limit)].shape[0]}")

# Tính Q1 VÀ Q3 và iqr (tứ phân vị)
Q3, Q1 = np.percentile(data['Age'], [75 ,25])
iqr = Q3-Q1
print(F'IQR = {iqr}\nQ1={Q1}\nQ3={Q3}')

# tính maximum và minimum limit
min_limit = Q1 - 1.5*iqr
print("Minimum limit:",min_limit)

max_limit = Q3 + 1.5*iqr
print("Maximum Limit:",max_limit)

# Impute outlier (Cho outliner = median)
data.loc[data['Age'] > max_limit,'Age']=data['Age'].median()
data.loc[data['Age'] < min_limit,'Age']=data['Age'].median()

##kiểm tra outliner
print(f"số outliers là: {data.loc[(data['Age'] > max_limit) | (data['Age'] < min_limit)].shape[0]}")

# Tính Q1 VÀ Q3 và iqr (tứ phân vị)
Q3, Q1 = np.percentile(data['monetary'], [75 ,25])
iqr = Q3-Q1
print(F'IQR = {iqr}\nQ1={Q1}\nQ3={Q3}')

# tính maximum và minimum limit
min_limit = Q1 - 1.5*iqr
print("Minimum limit:",min_limit)

max_limit = Q3 + 1.5*iqr
print("Maximum Limit:",max_limit)

# Impute outlier (Cho outliner = median)
data.loc[data['monetary'] > max_limit,'monetary']=data['monetary'].median()
data.loc[data['monetary'] < min_limit,'monetary']=data['monetary'].median()

##kiểm tra outliner
print(f"số outliers là: {data.loc[(data['monetary'] > max_limit) | (data['monetary'] < min_limit)].shape[0]}")

out = data[['standard_cost', 'product_first_sold_date', 'list_price', 'Profit', 'Age', 'monetary',
                'past_3_years_bike_related_purchases', 'tenure', 'frequency']]

plotnumber = 1
plt.figure(figsize=(15, 10))
for column in out.columns:
    if plotnumber <= 7: # set the limit
          plt.subplot(3, 3, plotnumber)# # hình gồm 3 hàng và 3 cột, plotnumber là thứ tự mỗi hình nhỏ bên trong
          sns.boxplot(data=out, x=column)# Plotting box plots để phát hiện outlier
          plt.xlabel(column, fontsize=10)  # Thay đổi cỡ chữ tại đây
          plotnumber += 1
plt.tight_layout()
plt.show()

"""# **Scaler, PCA và chia tập dữ liệu**"""

features = [ 'online_order', 'order_status', 'brand','product_line', 'owns_car', 'deceased_indicator',
            'wealth_segment','job_industry_category','job_title','gender', 'customer_rank', 'rank_r',
             'rank_f', 'rank_m', 'customer_segmentation_ranking', 'product_class', 'product_size', 'Age_Group',
             'rfm_score','standard_cost', 'product_first_sold_date', 'list_price', 'Profit', 'Age', 'monetary',
             'past_3_years_bike_related_purchases', 'tenure', 'frequency']

target = 'customer_segmentation'

X = data[features]
y = data[target]

from sklearn.preprocessing import MinMaxScaler
# Tạo một MinMaxScaler
scaler = MinMaxScaler()
# Chuẩn hóa dữ liệu sử dụng Min-Max Scaler
X_scaled = scaler.fit_transform(X)
# Chuyển đổi kết quả về dạng DataFrame
X_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)
# Xem 5 dòng đầu tiên của bảng RFM đã chuẩn hóa
X_scaled.head()

from sklearn.decomposition import PCA
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Create a pipeline with imputer and PCA
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('pca', PCA())
])

# Fit the pipeline on the scaled data with imputation
pipeline.fit(X_scaled)

# Access PCA attributes after fitting the pipeline
explained_variance_ratio = pipeline.named_steps['pca'].explained_variance_ratio_
cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)
n_components = np.argmax(cumulative_explained_variance_ratio >= 0.95) + 1

print(f"Số lượng thành phần chính cần thiết để giải thích 95% variance: {n_components}")

# Điền các giá trị thiếu sót (NaN) bằng giá trị trung bình
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X_scaled)  # Replace X_scaled with your data

# Áp dụng PCA
pca = PCA(n_components=17)
X_pca = pca.fit_transform(X_imputed)

plt.figure(figsize=(10, 5))  # Điều chỉnh kích thước của biểu đồ
# Vẽ countplot
sns.countplot(data=data, x='customer_segmentation')
# Xoay nhãn trên trục X
plt.xticks(rotation=90)
plt.show()

data.customer_segmentation.value_counts()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)

from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import GridSearchCV
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix

"""# **Random Forest**"""

# Khởi tạo mô hình Random Forest
random_forest = RandomForestClassifier()
# Thiết lập các giá trị để thử nghiệm
param_grid = {
    'n_estimators': [50, 100, 150],  # Các giá trị n_estimators để thử nghiệm
    'max_depth': [5, 10, 15],  # Các giá trị max_depth để thử nghiệm
    }
# Tìm kiếm siêu tham số tốt nhất
grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
# In ra bộ tham số tốt nhất
print("Bộ tham số tối ưu:", grid_search.best_params_)

model1 = RandomForestClassifier(n_estimators=150, max_depth=15, random_state=42)
model1.fit(X_train, y_train)

preds1 = model1.predict(X_test)
accuracy1 = accuracy_score(y_test, preds1)
print(f"Accuracy Model 1 (Random Forest): {accuracy1*100}%")
print("Classification Report Model 1 (Random Forest):")
print(classification_report(y_test, preds1))

# Tạo confusion matrix
conf_matrix = confusion_matrix(y_test, preds1,
                               labels=['Loosing Customer', 'Potential Customer', 'Recent Customer', 'Becoming Loyal',
                                       'High Risk Customer', 'Evasive Customer', 'Almost Lost Customer', 'Platinum Customer',
                                       'Very Loyal', 'Late Bloomer', 'Lost Customer'])

# Hiển thị confusion matrix dưới dạng heatmap với nhãn
plt.figure(figsize=(8, 6))
heatmap = sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges',
                      xticklabels=['Loosing Customer', 'Potential Customer', 'Recent Customer', 'Becoming Loyal',
                                   'High Risk Customer', 'Evasive Customer', 'Almost Lost Customer', 'Platinum Customer',
                                   'Very Loyal', 'Late Bloomer', 'Lost Customer'],
                      yticklabels=['Loosing Customer', 'Potential Customer', 'Recent Customer', 'Becoming Loyal',
                                   'High Risk Customer', 'Evasive Customer', 'Almost Lost Customer', 'Platinum Customer',
                                   'Very Loyal', 'Late Bloomer', 'Lost Customer'])
heatmap.set_xlabel('Dự đoán', color='red', fontsize=14)
heatmap.set_ylabel('Thực tế', color='red', fontsize=14)
plt.xlabel('Dự đoán')
plt.ylabel('Thực tế')
plt.title('Confusion Matrix của Random Forest Model', color='blue')
plt.show()

"""# **KNeighbors Classsifier**"""

# Khởi tạo mô hình
model = KNeighborsClassifier()
# Danh sách các tham số cần điều chỉnh
param_grid = {
    'n_neighbors': [3, 5, 7],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}
# GridSearchCV để tìm tham số tốt nhất
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)
# Hiển thị tham số tốt nhất
print("Best parameters found:")
print(grid_search.best_params_)

model2 = KNeighborsClassifier(metric='manhattan', n_neighbors=7, weights='distance')
model2.fit(X_train, y_train)

preds2 = model2.predict(X_test)
accuracy2 = accuracy_score(y_test, preds2)
print(f"Accuracy Model 2 (KNeighbors Classifier): {accuracy2*100}%")
print("Classification Report Model 2 (KNeighbors Classifier):")
print(classification_report(y_test, preds2))

# Tạo confusion matrix
conf_matrix2 = confusion_matrix(y_test, preds2,
                               labels=['Loosing Customer', 'Potential Customer', 'Recent Customer', 'Becoming Loyal',
                                       'High Risk Customer', 'Evasive Customer', 'Almost Lost Customer', 'Platinum Customer',
                                       'Very Loyal', 'Late Bloomer', 'Lost Customer'])

# Hiển thị confusion matrix dưới dạng heatmap với nhãn
plt.figure(figsize=(8, 6))
heatmap = sns.heatmap(conf_matrix2, annot=True, fmt='d', cmap='Blues',
                      xticklabels=['Loosing Customer', 'Potential Customer', 'Recent Customer', 'Becoming Loyal',
                                   'High Risk Customer', 'Evasive Customer', 'Almost Lost Customer', 'Platinum Customer',
                                   'Very Loyal', 'Late Bloomer', 'Lost Customer'],
                      yticklabels=['Loosing Customer', 'Potential Customer', 'Recent Customer', 'Becoming Loyal',
                                   'High Risk Customer', 'Evasive Customer', 'Almost Lost Customer', 'Platinum Customer',
                                   'Very Loyal', 'Late Bloomer', 'Lost Customer'])
heatmap.set_xlabel('Dự đoán', color='red', fontsize=14)
heatmap.set_ylabel('Thực tế', color='red', fontsize=14)
plt.xlabel('Dự đoán')
plt.ylabel('Thực tế')
plt.title('Confusion Matrix của KNeighbors Classifier Model', color='blue')
plt.show()

"""# **Naive Bayes**"""

# Khởi tạo mô hình Naive Bayes
models = GaussianNB()
# Danh sách các tham số cần điều chỉnh
param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}
# GridSearchCV để tìm tham số tốt nhất
grid_search = GridSearchCV(estimator=models, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)
# Hiển thị tham số tốt nhất
print("Best parameters found:")
print(grid_search.best_params_)

# Khởi tạo mô hình Naive Bayes với tham số var_smoothing
model3 = GaussianNB(var_smoothing=1e-9)

# Huấn luyện mô hình trên tập huấn luyện
model3.fit(X_train, y_train)

preds3 = model3.predict(X_test)
accuracy3 = accuracy_score(y_test, preds3)
print(f"Accuracy Model 3 (Naive Bayes): {accuracy3*100}%")
print("Classification Report Model 3 (Naive Bayes):")
print(classification_report(y_test, preds3))

# Tạo DataFrame từ dự đoán và thực tế
df_predictions = pd.DataFrame({'Thực tế': y_test, 'Dự đoán': preds3})
# Tính toán dự đoán chính xác và dự đoán sai của mô hình
correct_predictions = (df_predictions['Thực tế'] == df_predictions['Dự đoán']).sum()
total_predictions = len(df_predictions)
incorrect_predictions = total_predictions - correct_predictions
# Hiển thị số liệu về dự đoán
print(f'Tổng số dự đoán: {total_predictions}')
print(f'Tổng số dự đoán đúng: {correct_predictions}')
print(f'Tổng số dự đoán sai: {incorrect_predictions}')

"""# **Đánh giá ba mô hình**"""

from sklearn.model_selection import cross_val_score
# Khởi tạo các mô hình
random_model = RandomForestClassifier()
kneighbors_model = KNeighborsClassifier()
bayes_model = GaussianNB()

# Thực hiện kiểm định chéo với các thông số đánh giá biến phân loại
random_scores = cross_val_score(random_model, X_pca, y, cv=5, scoring='accuracy')
kneighbors_scores = cross_val_score(kneighbors_model, X_pca, y, cv=5, scoring='accuracy')
bayes_scores = cross_val_score(bayes_model, X_pca, y, cv=5, scoring='accuracy')

# In ra kết quả kiểm định chéo với các thông số đánh giá biến phân loại cho từng mô hình
print(f"Điểm số kiểm định chéo Random Forest: {random_scores.mean()*100:.2f}%")
print(f"Điểm số kiểm định chéo KNeighbors Classifiers: {kneighbors_scores.mean()*100:.2f}%")
print(f"Điểm số kiểm định chéo Naive Bayes: {bayes_scores.mean()*100:.2f}%")

# Điểm số kiểm định chéo của các mô hình
model_scores = [random_scores.mean(), kneighbors_scores.mean(), bayes_scores.mean()]
model_names = ['Random Forest', 'KNeighbors Classifiers', 'Naive Bayes']

# Vẽ biểu đồ
plt.figure(figsize=(8, 6))
plt.bar(model_names, model_scores, color=['blue', 'green', 'orange'])
plt.xlabel('Mô hình', color='red')
plt.ylabel('Điểm số kiểm định chéo', color='red')
plt.title('So sánh điểm số kiểm định chéo của các mô hình', color='red')
plt.ylim(0, 1)
plt.show()

"""Random Forest có điểm số kiểm định cao nhất (99.35%), cho thấy mô hình này có khả năng dự đoán tốt trên dữ liệu kiểm tra.
KNeighbors Classifiers có điểm số kiểm định thấp hơn nhiều so với Random Forest (73.81%), có thể là do cách thức hoạt động của mô hình không phù hợp hoặc không phản ánh tốt dữ liệu.
Naive Bayes có điểm số kiểm định khá cao (90.74%), tuy không cao bằng Random Forest, nhưng vẫn là một kết quả khả quan.
Dựa trên các điểm số kiểm định chéo, có thể kết luận rằng Random Forest hiện đang là mô hình có hiệu suất tốt nhất trong số các mô hình bạn đã đánh giá. Có thể do Random Forest có khả năng tìm ra các mối quan hệ phức tạp giữa các biến đầu vào và đầu ra tốt hơn so với KNeighbors và Naive Bayes trong trường hợp dữ liệu cụ thể này.
Mục tiêu của bài toán là tính toán những yếu tố và dự đoán phân khúc khách hàng, nên yếu tố chính xác là yếu tố được chú ý. Vì vậy, mô hình tối ưu được lựa chọn cho bộ dữ liệu trong ba mô hình máy học là Random Forest.  

"""